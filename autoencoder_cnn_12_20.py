# -*- coding: utf-8 -*-
"""AutoEncoder_CNN_12*20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IN_WY0Xh1JScEHWjGKJ9nBu4_YWw8cWA
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from sklearn.neighbors import KernelDensity

from google.colab import drive
drive.mount('/content/drive')

normal_path = "/content/drive/MyDrive/IOT_Project/data/normal/"
fog_path = "/content/drive/MyDrive/IOT_Project/data/fog/"

"""## 1.前處理

### 1.1合併所有normal
"""

def preprocess(folder_path):
    files = os.listdir(folder_path)
    result_df = pd.read_csv(f'{folder_path}{files[0]}', header=None)
    for file in files[1:]:
        df_temp = pd.read_csv(f'{folder_path}{file}', header=None)
        result_df = pd.concat([result_df, df_temp]) # 合併資料結內所有 dataframe

    return result_df

result_df = preprocess(normal_path)
print(result_df)
print(len(result_df))

result_df

# 要插入全為0的新欄位
rows = len(result_df)
new_column = pd.Series([0 for i in range(rows)])
# 新欄位索引
insert_list = [20, 40, 60, 61, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 117, 118, 119, 120,
         140, 160, 180, 181, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230] # 237, 238, 239, 240]

# print(len(insert_list))
for i in range(len(insert_list)):
  result_df.insert(loc=insert_list[i], column=f'{insert_list[i]}', value=new_column)

# result_df['237']=0

result_df['237'] = 0
result_df['238'] = 0
result_df['239'] = 0
result_df['240'] = 0

result_df

# 把240個欄位名稱依序改成改成0~240
result_df.columns = range(240)
result_df

# insert_list = [20, 40, 60, 61, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 117, 118, 119, 120,
#          140, 160, 180, 181, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 237, 238, 239, 240]

# print(len(insert_list))

total = len(result_df)
combined_list = []
for i in range(total):
  row = result_df.iloc[i].tolist()
  combined_list.append(row)

print(len(combined_list))
print(combined_list[:10])

combined_array = np.array(combined_list)
print(combined_array.shape)

"""### 轉成圖像形式"""

combined_array = combined_array.reshape(35350, 12, 20)
print(combined_array.shape)

print(combined_array[0])
print(combined_array[0].shape)

"""## 2.模型調適

"""

# 将数据转换为适当的形状 (35350, 12, 20, 1)，添加通道维度
samples = combined_array.reshape(35350, 12, 20, 1)/255

# 构建卷积自编码器
model = models.Sequential()

# 编码器部分
model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=(12, 20, 1)))
# model.add(layers.MaxPooling2D((2, 2), padding='same'))
# model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
# model.add(layers.MaxPooling2D((2, 2), padding='same'))

# 解码器部分
# model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
# model.add(layers.UpSampling2D((2, 2)))
model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
# model.add(layers.UpSampling2D((2, 2)))

# 输出层
model.add(layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))

learning_rate = 0.0001  # You can adjust this value
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])

# 训练模型
model.fit(samples, samples, epochs=50, batch_size=1000, shuffle=True)

# 使用训练好的模型进行预测
decoded_samples = model.predict(samples)

# 计算重构误差
mse = tf.keras.losses.MeanSquaredError()
reconstruction_error = mse(samples, decoded_samples).numpy()
print(f"Reconstruction Error: {reconstruction_error}")

# 可以使用 reconstruction_error 进行异常检测，设置一个阈值来判断何时认为是异常

# 假设你有一个单独的样本，形状为 (12, 20)
# single_sample = np.random.rand(12, 20)  # 你应该替换这行代码，使用你的实际数据
single_sample = combined_array[0]/255

# 将样本转换为适当的形状 (1, 12, 20, 1)，添加通道维度
single_sample = single_sample.reshape(1, 12, 20, 1)

# 使用训练好的模型进行预测
decoded_single_sample = model.predict(single_sample)

# 计算重构误差
reconstruction_error_single_sample = mse(single_sample, decoded_single_sample).numpy()
print(f"Reconstruction Error for Single Sample: {reconstruction_error_single_sample}")

# 假设你有一个单独的样本，形状为 (12, 20)
# single_sample = np.random.rand(12, 20)  # 你应该替换这行代码，使用你的实际数据
single_sample = combined_array[0]/255

# 将样本转换为适当的形状 (1, 12, 20, 1)，添加通道维度
single_sample = single_sample.reshape(1, 12, 20, 1)

# 使用训练好的模型进行预测
decoded_single_sample = model.predict(single_sample)

abs_diff = np.abs(single_sample - decoded_single_sample)


# 计算重构误差
print("絕對值差異：")
print(abs_diff)
print(abs_diff.shape)
print(np.mean(abs_diff))

total = combined_array.shape[0]

reconstruction_normal_list = []
for i in range(600):
  single_sample = combined_array[i]/255
  single_sample = single_sample.reshape(1, 12, 20, 1)
  decoded_single_sample = model.predict(single_sample)
  # reconstruction_error_single_sample = mse(single_sample, decoded_single_sample).numpy()
  reconstruction_error_single_sample = np.mean(np.square(single_sample - decoded_single_sample))
  reconstruction_normal_list.append(reconstruction_error_single_sample * 1000)

print(reconstruction_normal_list[:60])

"""## 3.異常檢測"""

# 讀取異常數據
result_df = preprocess(fog_path)

# 要插入全為0的新欄位
rows = len(result_df)
new_column = pd.Series([0 for i in range(rows)])
# 新欄位索引
insert_list = [20, 40, 60, 61, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 117, 118, 119, 120,
         140, 160, 180, 181, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230] # 237, 238, 239, 240]

# print(len(insert_list))
for i in range(len(insert_list)):
  result_df.insert(loc=insert_list[i], column=f'{insert_list[i]}', value=new_column)

# result_df['237']=0

result_df['237'] = 0
result_df['238'] = 0
result_df['239'] = 0
result_df['240'] = 0

# 把240個欄位名稱依序改成改成0~240
result_df.columns = range(240)
result_df

total = len(result_df)
combined_list = []
for i in range(total):
  row = result_df.iloc[i].tolist()
  combined_list.append(row)

print(len(combined_list))
print(combined_list[:10])

combined_fog_array = np.array(combined_list)
print(combined_fog_array.shape)

combined_fog_array = combined_fog_array.reshape(combined_fog_array.shape[0], 12, 20)
print(combined_fog_array.shape)

print(combined_fog_array[0])
print(combined_fog_array[0].shape)

single_sample = combined_fog_array[555]/255

# 将样本转换为适当的形状 (1, 12, 20, 1)，添加通道维度
single_sample = single_sample.reshape(1, 12, 20, 1)

# 使用训练好的模型进行预测
decoded_single_sample = model.predict(single_sample)

# 计算重构误差
reconstruction_error_single_sample = mse(single_sample, decoded_single_sample).numpy()
print(f"Reconstruction Error for Single Sample: {reconstruction_error_single_sample}")

total = combined_fog_array.shape[0]

reconstruction_fog_list = []
for i in range(600):
  single_sample = combined_fog_array[i]/255
  single_sample = single_sample.reshape(1, 12, 20, 1)
  decoded_single_sample = model.predict(single_sample)
  # print('decoded_single_sample = ', decoded_single_sample)
  # 計算重建誤差
  reconstruction_error_single_sample = np.mean(np.square(single_sample - decoded_single_sample))
  # reconstruction_error_single_sample = mse(single_sample, decoded_single_sample).numpy()
  reconstruction_fog_list.append(reconstruction_error_single_sample * 1000)

print(reconstruction_fog_list[:10])

print(reconstruction_normal_list[:60])
print(reconstruction_fog_list[:60])

plt.plot([i for i in range(len(reconstruction_normal_list))], reconstruction_normal_list, color='blue', label='normal')
plt.plot([i for i in range(len(reconstruction_fog_list))], reconstruction_fog_list, color='red', label='fog')
plt.legend()
plt.show()

"""### 隨機生成的壓力點位"""

single_sample = np.random.rand(12, 20)/255

# 将样本转换为适当的形状 (1, 12, 20, 1)，添加通道维度
single_sample = single_sample.reshape(1, 12, 20, 1)

# 使用训练好的模型进行预测
decoded_single_sample = model.predict(single_sample)

 # 計算重建誤差
reconstruction_error_single_sample = np.mean(np.square(single_sample - decoded_single_sample))
print(f"Reconstruction Error for Single Sample: {reconstruction_error_single_sample * 1000}")